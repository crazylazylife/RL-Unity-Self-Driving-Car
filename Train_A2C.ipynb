{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys, os, cv2, random\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "\"\"\" More essential libraries for the task \"\"\"\n",
    "from gym_unity.envs import UnityEnv\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#Importing Keras esssentials\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:mlagents_envs:Connected new brain:\nCarBrain?team=0\nINFO:gym_unity:1 agents within environment.\n<UnityEnv instance>\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just some fancy folder names given to remember the various versions of the environment: NewSETAgent_<terrain_size>_<target_random/fixed>_<other_updates>\\ModifiedFinalSEt\n",
    "\"\"\"\n",
    "env = UnityEnv(\"NewSETAgent_300_RandomTarget_NewReward\\ModifiedFinalSEt\", use_visual=True, uint8_visual=True)\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' Code to test if the environment is running. Taken form the original documentation of Unity ML-Agents ver 0.15 '"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\"\"\" Code to test if the environment is running. Taken form the original documentation of Unity ML-Agents ver 0.15 \"\"\"\n",
    "\n",
    "# for episode in range(1):\n",
    "#     initial_observation = env.reset()\n",
    "#     done = False\n",
    "#     episode_rewards = 0\n",
    "#     step = 0\n",
    "#     while step < 10:\n",
    "#         observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#         plt.imshow(observation[:, :, 0])\n",
    "#         #print(observation)\n",
    "#         print(step,\" \",reward)\n",
    "#         step += 1\n",
    "#         episode_rewards += reward\n",
    "#     print(\"Total reward this episode: {}\".format(episode_rewards))\n",
    "\n",
    "# initial_observation = env.reset()\n",
    "# print(initial_observation.reshape(16384))\n",
    "# observation_examples = np.array([env.observation_space.sample().reshape(16384) for x in range(10000)])\n",
    "# print(observation_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-2\n",
    "gamma_ = 0.95\n",
    "frame = 0\n",
    "num_episodes = 1000\n",
    "load_model = False\n",
    "#episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 64, 1)\n(2,)\n[-1. -1.]\n[1. 1.]\n"
    }
   ],
   "source": [
    "input_dim = env.observation_space.shape\n",
    "print(input_dim)\n",
    "print(env.action_space.shape)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)\n",
    "\n",
    "\"\"\" Setting the input tensor \"\"\"\n",
    "visible = Input(shape=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nTo be executed the first time after installing Graphviz. Ignore, if already done. Graphviz is used to output the image of the network models used for the actor and critic agent.\\n'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\"\"\"\n",
    "To be executed the first time after installing Graphviz. Ignore, if already done. Graphviz is used to output the image of the network models used for the actor and critic agent.\n",
    "\"\"\"\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:tensorflow:From C:\\Users\\himad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From <ipython-input-8-28637fce48f5>:14: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\nInstructions for updating:\nThe TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\nWARNING:tensorflow:From C:\\Users\\himad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\distributions\\normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\nInstructions for updating:\nThe TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 64, 64, 1)    0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 62, 62, 32)   320         input_1[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 32)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 18, 18, 32)   9248        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 32)     0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 1152)         0           max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 20)           23060       flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 20)           420         dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 2)            42          dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 2)            42          dense_2[0][0]                    \n==================================================================================================\nTotal params: 33,132\nTrainable params: 33,132\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "\"\"\" Actor Network \"\"\"\n",
    "with tf.compat.v1.variable_scope(\"ActorNetwork\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "    ac_conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "    ac_pool1 = MaxPooling2D(pool_size=(3, 3))(ac_conv1)\n",
    "    ac_conv2 = Conv2D(32, kernel_size=3, activation='relu')(ac_pool1)\n",
    "    ac_pool2 = MaxPooling2D(pool_size=(3, 3))(ac_conv2)\n",
    "    ac_flat = Flatten()(ac_pool2)\n",
    "    ac_hidden1 = Dense(20, activation='relu')(ac_flat)\n",
    "    ac_hidden2 = Dense(20, activation='relu')(ac_hidden1)\n",
    "    mu_output = Dense(2, activation='softmax')(ac_hidden2)\n",
    "    sigma_output = Dense(2, activation = 'softmax')(ac_hidden2)\n",
    "    \n",
    "    normal_dist = tf.compat.v1.distributions.Normal(loc = mu_output, scale=sigma_output)\n",
    "    act_out = tf.keras.backend.reshape(normal_dist.sample(1), shape=[-1,2])\n",
    "    act_out = tf.clip_by_value(act_out, clip_value_min = env.action_space.low, clip_value_max = env.action_space.high)\n",
    "    \n",
    "    actor_model = Model(inputs=visible, outputs=[mu_output, sigma_output])\n",
    "    #PLotting the model\n",
    "    plot_model(actor_model, to_file='ActorNetwork_CNN.png')\n",
    "    #Model summary\n",
    "    actor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 64, 64, 1)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 62, 62, 32)        320       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 20, 20, 32)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 18, 18, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 6, 32)          0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 1152)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 20)                23060     \n_________________________________________________________________\ndense_6 (Dense)              (None, 20)                420       \n_________________________________________________________________\ndense_7 (Dense)              (None, 2)                 42        \n=================================================================\nTotal params: 33,090\nTrainable params: 33,090\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "\"\"\" Critic Network \"\"\"\n",
    "with tf.compat.v1.variable_scope(\"CriticNetwork\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "    ct_conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "    ct_pool1 = MaxPooling2D(pool_size=(3, 3))(ct_conv1)\n",
    "    ct_conv2 = Conv2D(32, kernel_size=3, activation='relu')(ct_pool1)\n",
    "    ct_pool2 = MaxPooling2D(pool_size=(3, 3))(ct_conv2)\n",
    "    ct_flat = Flatten()(ct_pool2)\n",
    "    ct_hidden1 = Dense(20, activation='relu')(ct_flat)\n",
    "    ct_hidden2 = Dense(20, activation='relu')(ct_hidden1)\n",
    "    ct_output = Dense(2, activation='softmax')(ct_hidden2)\n",
    "    critic_model = Model(inputs=visible, outputs=ct_output)\n",
    "    #PLotting the model\n",
    "    plot_model(critic_model, to_file='CriticNetwork_CNN.png')\n",
    "    #Model Summary\n",
    "    critic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_ = tf.compat.v1.placeholder(tf.float32, [None, 2])\n",
    "returns_ = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "logprobs = normal_dist.log_prob(actions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = normal_dist.entropy()\n",
    "advantages = returns_ - ct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = tf.reduce_mean(-logprobs * advantages - 0.01*entropy)\n",
    "value_loss = tf.reduce_mean(tf.square(returns_ - ct_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_policy = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_actor)\n",
    "optimizer_value = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy = optimizer_policy.minimize( policy_loss,var_list=tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, \"ActorNetwork\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_value = optimizer_value.minimize( value_loss,var_list=tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, \"CriticNetwork\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Not loading any model for now!\n"
    }
   ],
   "source": [
    "\"\"\" Execute it to load saved weights \"\"\"\n",
    "if load_model:\n",
    "    actor_model.load_weights(\"./a2c_actor.h5\")\n",
    "    critic_model.load_weights(\"./a2c_critic.h5\")\n",
    "    print(\"Saved model loaded...\\nStart Training.\")\n",
    "else:\n",
    "    print(\"Not loading any model for now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.compat.v1.summary.FileWriter(\"./tensorboard2/\")\n",
    "tf.compat.v1.summary.scalar(\"Policy_Loss\", policy_loss)\n",
    "tf.compat.v1.summary.scalar(\"Value_Loss\", value_loss)\n",
    "write_op = tf.compat.v1.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 73,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 74,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 75,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 76,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 77,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 78,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 79,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 80,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 81,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 82,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 83,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 84,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 85,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 86,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 87,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 88,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 89,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 90,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 91,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 92,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 93,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 94,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 95,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 96,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 97,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 98,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 99,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 100,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 101,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 102,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 103,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 104,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 105,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 106,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 107,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 108,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 109,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 110,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 111,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 112,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 113,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 114,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 115,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 116,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 117,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 118,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 119,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 120,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 121,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 122,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 123,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 124,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 125,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 126,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 127,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 128,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 129,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 130,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 131,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 132,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 133,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 134,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 135,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 136,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 137,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 138,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 139,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 140,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 141,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 142,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 143,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 144,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 145,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 146,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 147,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 148,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 149,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 150,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 151,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 152,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 153,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 154,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 155,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 156,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 157,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 158,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 159,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 160,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 161,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 162,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n\nEpisode : 163,\nMean mu : nan, Min mu : nan, Max mu : nan, Median mu : nan\nMean sigma : nan, Min sigma : nan, Max sigma : nan, Median sigma : nan\nScores : -0.6000, Max reward : 0.0000, Min reward : -0.6000\nModel saved.\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-11a554758faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmu_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_out\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mvisible\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcurr_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mnext_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mep_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mcurr_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gym_unity\\envs\\__init__.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_step_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mn_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicationException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has stopped.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mlaunched\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             raise UnityTimeOutException(\n\u001b[0;32m     94\u001b[0m                 \u001b[1;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    857\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Ultimate code to train the model \"\"\"\n",
    "for i_ep in range(11, num_episodes):\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    rewards_list = []\n",
    "    mu_list = []\n",
    "    sigma_list = []\n",
    "    ep_reward = 0\n",
    "    ep_rewards = []\n",
    "    step = 0\n",
    "    curr_frame = env.reset()\n",
    "    done = False\n",
    "    curr_time = time()\n",
    "    while True:\n",
    "        curr_state = np.array(curr_frame.reshape(-1, 64, 64, 1))\n",
    "#       curr_state = (curr_state - env.observation_space.low) / (env.observation_space.high - env.observation_space.low)\n",
    "#       curr_state = tf.convert_to_tensor(curr_state, dtype = tf.float32)\n",
    "#       print(curr_state.shape)\n",
    "#       print(curr_state.dtype)\n",
    "#       curr_state = curr_state.astype(np.float32)\n",
    "#       curr_state = np.float_(curr_state)\n",
    "#       print(curr_state.dtype)\n",
    "#       print(states_.shape)\n",
    "#       tf.image.convert_image_dtype(curr_state, dtype = tf.float32)\n",
    "\n",
    "        mu, sigma, action = sess.run([mu_output, sigma_output, act_out], feed_dict={visible: curr_state})\n",
    "        next_frame, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        curr_frame = next_frame\n",
    "\n",
    "        reward_t = reward\n",
    "        #if step % 100 == 0:\n",
    "            #print(\"Episode:  \"+str(i_ep)+\" Done: \"+str(done)+\" Step:  \"+str(step)+\" Reward: \"+str(reward)+\" Cumulative Reward: \"+str(ep_reward)+\" Action: \"+str(action))\n",
    "        step += 1\n",
    "        states_list.append(curr_state)\n",
    "        actions_list.append(action)\n",
    "        rewards_list.append(reward_t)\n",
    "        \n",
    "        mu_list.append(mu.reshape(-1,))\n",
    "        sigma_list.append(sigma.reshape(-1,))\n",
    "        #vprint(\"Done till here, Done: \", done)\n",
    "        \"\"\" Ending an episode after 1000 steps are taken. Figured out 500 - 600 steps are enough to reach the target. However, if you increase the tea=rrain, the value might change. So its better to remove the restriction in most cases.\"\"\"\n",
    "        # if step == 1000:\n",
    "        #     done = True\n",
    "\n",
    "        if done:\n",
    "            #vprint(\"Episode:  \"+str(i_ep)+\" Done: \"+str(done)+\" Step:  \"+str(step)+\" Reward: \"+str(reward)+\" Cumulative Reward: \"+str(ep_reward)+\" Action: \"+str(action))\n",
    "            states = np.vstack(states_list)\n",
    "            actions = np.vstack(actions_list)\n",
    "            rewards = np.hstack(rewards_list)\n",
    "            mus = np.hstack(mu_list)\n",
    "            sigmas = np.hstack(sigma_list)\n",
    "\n",
    "            returns = np.zeros_like(rewards)\n",
    "            rolling = 0\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                rolling = rolling * gamma_ + rewards[i]\n",
    "                returns[i] = rolling\n",
    "            returns -= np.mean(returns)\n",
    "            returns /= np.std(returns)\n",
    "\n",
    "            feed_dict = {visible: states, actions_: actions, returns_: returns.reshape(-1, 1)}\n",
    "\n",
    "            sess.run(train_value, feed_dict=feed_dict)\n",
    "            sess.run(train_policy, feed_dict=feed_dict)\n",
    "            \n",
    "            summary = sess.run(write_op, feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, i_ep)\n",
    "            writer.flush()\n",
    "\n",
    "            print(\"\\nEpisode : %s,\" % (i_ep) + \\\n",
    "                   \"\\nMean mu : %.5f, Min mu : %.5f, Max mu : %.5f, Median mu : %.5f\" % \\\n",
    "                       (np.nanmean(mus), np.min(mus), np.max(mus), np.median(mus)) + \\\n",
    "                   \"\\nMean sigma : %.5f, Min sigma : %.5f, Max sigma : %.5f, Median sigma : %.5f\" % \\\n",
    "                       (np.nanmean(sigmas), np.min(sigmas), np.max(sigmas), np.median(sigmas)) + \\\n",
    "                   \"\\nScores : %.4f, Max reward : %.4f, Min reward : %.4f\" % (ep_reward, np.max(rewards), np.min(rewards)))\n",
    "            \n",
    "            actor_model.save_weights(\"./a2c_actor.h5\")\n",
    "            critic_model.save_weights(\"./a2c_critic.h5\")\n",
    "            print(\"Model saved.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).\n"
    }
   ],
   "source": [
    "\"Very Essential. Please close the environemt. PLEASE\"\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitab285c127be24f6ab5669940295e5be2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}